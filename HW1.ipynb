{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linjiw/linji85/blob/main/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO24x_Pxvrc3"
      },
      "source": [
        "##log in kaggle -> download dataset -> unzip dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-VzCyWSvjnb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "TOKEN = {\"username\":\"xxx\",\"key\":\"xxx\"}\n",
        "\n",
        "! pip install kaggle==1.5.12\n",
        "! mkdir -p .kaggle\n",
        "! mkdir -p /content & mkdir -p /content/.kaggle & mkdir -p /root/.kaggle/\n",
        "\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(TOKEN, file)\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! ls \"/content/.kaggle\"\n",
        "! chmod 600 /content/.kaggle/kaggle.json\n",
        "! cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "\n",
        "! kaggle config set -n path -v /content\n",
        "\n",
        "! kaggle competitions download -c 11-785-s22-hw1p2\n",
        "\n",
        "! unzip competitions/11-785-s22-hw1p2/11-785-s22-hw1p2.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4zBiA2qwBLb"
      },
      "source": [
        "##mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA4qv2BXwBjq",
        "outputId": "91ea7551-4ed0-498d-b67f-71567447ad93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I use wand to log my experiment process."
      ],
      "metadata": {
        "id": "KqPF60kVIRfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "MgKyl1p96gCs",
        "outputId": "a9964699-a3b3-4e64-a292-17f0b79738ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlmGTucGwKBk"
      },
      "source": [
        "##import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yLW73YEwEnO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkTuhPzVwUnJ"
      },
      "source": [
        "##define model architechture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model trainning log\n",
        "\n",
        "### Increase model size\n",
        "For the model architechture, I tried a lot from scratch. \n",
        "Frist I use a simple MLP with just three layers. And that cannot work well. Then after calculated the input layer is about 273, which would a large hidden layer should bring performance increase. Then I started to add layers and layer size. The performace starts increase to 79.\n",
        "\n",
        "And then I found that I have not use the whole train dataset, that might be a reason why the performance is not good enought. After moving to the whole dataset, the performance has increased to 81.\n",
        "\n",
        "\n",
        "### Increase context size\n",
        "Then I started to try other parameters, first I increased my context to 30, and this brought slightly performance increase. Then I did ablation study for the context, and found that context around 25-30 could bring good results.\n",
        "\n",
        "\n",
        "### Use cylinder architechture\n",
        "After stay the context to 30, I noticed that students in piazza claims that cylinder architecther usually have better performance, then I replaced my neurons for each layer to 4096 and stay keeped other layers in 2048 which still is a great number. This brought me to around 84. \n",
        "\n",
        "\n",
        "### Change dropout value to avoid overfitting\n",
        "Also, in the process I played with the dropout values and found that small dropout value could increase the training speed but will easily bring my model to overfitting. Thus, I found that I could lower my learning rate before the overfitting happens, and this bring a good results.\n",
        "\n",
        "\n",
        "### reduce learning rate\n",
        "However, 84 is not enoough to get full credit. Then I started to turn my learning rate much lower when my model reached to 84. The learning rate I use is 0.0001 and it could keep my model increase to 85, which is a great results. \n",
        "\n",
        "### R-Dropout\n",
        "Beyound these, I also implemented the R-Dropout to calculate the loss, however, since I did not do an abaltion study for that, I cannot report the performance for that."
      ],
      "metadata": {
        "id": "pUQxjR57IanI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0QJil6ZwTh4"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,context=30,dropout=0.4):\n",
        "        super(Network, self).__init__()\n",
        "        in_size = 13 * (1+2*context)\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_features=in_size,out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.BatchNorm1d(num_features=4096),\n",
        "            nn.Linear(in_features=4096,out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.BatchNorm1d(num_features=4096),\n",
        "            nn.Linear(in_features=4096,out_features=2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.BatchNorm1d(num_features=2048),\n",
        "            nn.Linear(in_features=2048,out_features=2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.BatchNorm1d(num_features=2048),\n",
        "            nn.Linear(in_features=2048,out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.BatchNorm1d(num_features=1024),\n",
        "            nn.Linear(in_features=1024,out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.BatchNorm1d(num_features=1024),\n",
        "            nn.Linear(in_features=1024,out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.Linear(in_features=512,out_features=40),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.BatchNorm1d(num_features=40),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # self.laysers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.layers(A0)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZwdgzGxzfYj",
        "outputId": "83ef6399-c200-4227-b114-1849857b32b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=273, out_features=4096, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.4, inplace=False)\n",
              "    (3): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Dropout(p=0.4, inplace=False)\n",
              "    (7): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): Linear(in_features=4096, out_features=2048, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): Dropout(p=0.4, inplace=False)\n",
              "    (11): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (13): ReLU()\n",
              "    (14): Dropout(p=0.4, inplace=False)\n",
              "    (15): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Dropout(p=0.4, inplace=False)\n",
              "    (19): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (21): ReLU()\n",
              "    (22): Dropout(p=0.4, inplace=False)\n",
              "    (23): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (25): ReLU()\n",
              "    (26): Dropout(p=0.4, inplace=False)\n",
              "    (27): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (28): Linear(in_features=512, out_features=40, bias=True)\n",
              "    (29): ReLU()\n",
              "    (30): Dropout(p=0.4, inplace=False)\n",
              "    (31): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "Network(10,dropout=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdVMX4LX1Z0D"
      },
      "source": [
        "##Dataset function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTZfRwWa0xdk"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        self.sample = sample \n",
        "        \n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.Y_dir = data_path + \"/\" + partition +\"/transcript/\"\n",
        "        \n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "        self.Y_names = os.listdir(self.Y_dir)\n",
        "\n",
        "        # using a small part of the dataset to debug\n",
        "        if csvpath:\n",
        "            subset = self.parse_csv(csvpath)\n",
        "            self.X_names = [i for i in self.X_names if i in subset]\n",
        "            self.Y_names = [i for i in self.Y_names if i in subset]\n",
        "        \n",
        "        if shuffle == True:\n",
        "            XY_names = list(zip(self.X_names, self.Y_names))\n",
        "            random.shuffle(XY_names)\n",
        "            self.X_names, self.Y_names = zip(*XY_names)\n",
        "        \n",
        "        assert(len(self.X_names) == len(self.Y_names))\n",
        "        self.length = len(self.X_names)\n",
        "        \n",
        "        self.PHONEMES = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n",
        "      \n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[1])\n",
        "        return subset[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.length / self.sample))\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n",
        "        \n",
        "        X, Y = [], []\n",
        "        for j in sample_range:\n",
        "            X_path = self.X_dir + self.X_names[j]\n",
        "            Y_path = self.Y_dir + self.Y_names[j]\n",
        "            \n",
        "            label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n",
        "\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "            Y.append(np.array(label))\n",
        "            \n",
        "        X, Y = np.concatenate(X), np.concatenate(Y)\n",
        "        return X, Y\n",
        "    \n",
        "class LibriItems(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y, context = 0):\n",
        "        assert(X.shape[0] == Y.shape[0])\n",
        "        \n",
        "        self.length  = X.shape[0]\n",
        "        # print(f\"self.length {self.length}\")\n",
        "        self.context = context\n",
        "        empty_matrix = np.zeros((context,13))\n",
        "        if context == 0:\n",
        "            self.X, self.Y = X, Y\n",
        "        else:\n",
        "            # TODO: self.X, self.Y = ... \n",
        "            self.X = np.concatenate((empty_matrix,X,empty_matrix ))\n",
        "            self.Y = Y\n",
        "            pass \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        if self.context == 0:\n",
        "            xx = self.X[i].flatten()\n",
        "            yy = self.Y[i]\n",
        "        else:\n",
        "            # TODO xx, yy = ...\n",
        "            # xx = self.X[self.context:]\n",
        "            xx = self.X[i:i+2*self.context+1].flatten()\n",
        "            # print(f\"xx.shape {xx.shape} pos {i}\")\n",
        "\n",
        "            yy = self.Y[i]\n",
        "            pass\n",
        "        return xx, yy\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CB_FKK_1krX"
      },
      "source": [
        "##train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMo7IZCq1jLL"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "def compute_kl_loss( p, q, pad_mask=None):\n",
        "    \n",
        "    p_loss = torch.nn.functional.kl_div(torch.nn.functional.log_softmax(p, dim=-1), torch.nn.functional.softmax(q, dim=-1), reduction='none')\n",
        "    q_loss = torch.nn.functional.kl_div(torch.nn.functional.log_softmax(q, dim=-1), torch.nn.functional.softmax(p, dim=-1), reduction='none')\n",
        "    \n",
        "    # pad_mask is for seq-level tasks\n",
        "    if pad_mask is not None:\n",
        "        p_loss.masked_fill_(pad_mask, 0.)\n",
        "        q_loss.masked_fill_(pad_mask, 0.)\n",
        "\n",
        "    # You can choose whether to use function \"sum\" and \"mean\" depending on your task\n",
        "    p_loss = p_loss.mean()\n",
        "    q_loss = q_loss.mean()\n",
        "\n",
        "    loss = (p_loss + q_loss) / 2\n",
        "    return loss\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.parameters(),\n",
        "                               lr=learning_rate)\n",
        "    return optimizer\n",
        "def build_network(conext=30, dropout=0.5):\n",
        "    network = Network(conext,dropout)\n",
        "    # network.load_state_dict(torch.load(\"/content/drive/MyDrive/11785/model/model_epoch_acc_81.43418102540598_12_16_02_2022_11_52_06.pth\"))\n",
        "    return network.to(device)\n",
        "\n",
        "\n",
        "def train(args, model, device, train_samples, optimizer,scheduler, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    # print(f\"train_samples.shape: {train_samples.shape}\")\n",
        "    # print(4)\n",
        "    \n",
        "    # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "\n",
        "    for i in trange(len(train_samples)):\n",
        "        X, Y = train_samples[i]\n",
        "        train_items = LibriItems(X, Y, context=args['context'])\n",
        "        # print(5)\n",
        "        train_loader = torch.utils.data.DataLoader(train_items, batch_size=batch_size, shuffle=True)\n",
        "        # print(f\"X.shape: {X.shape}\")\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # print(f\"batch_idx: {batch_idx} data {data.shape}\")\n",
        "            data = data.float().to(device)\n",
        "            target = target.long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            logits = model(data)\n",
        "            \n",
        "            logits2 = model(data)\n",
        "\n",
        "            # cross entropy loss for classifier\n",
        "            ce_loss = 0.5 * (criterion(logits, target) + criterion(logits2, target))\n",
        "\n",
        "            kl_loss = compute_kl_loss(logits, logits2)\n",
        "\n",
        "            # carefully choose hyper-parameters\n",
        "            loss = ce_loss + 5 * kl_loss\n",
        "\n",
        "            # loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % args['log_interval'] == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                  100. * batch_idx / len(train_loader), loss.item()))\n",
        "    # scheduler.step()\n",
        "    \n",
        "    return loss.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "def test(args, model, device, dev_samples,criterion):\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in trange(len(dev_samples)):\n",
        "            X, Y = dev_samples[i]\n",
        "\n",
        "            test_items = LibriItems(X, Y, context=args['context'])\n",
        "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for data, true_y in test_loader:\n",
        "                data = data.float().to(device)\n",
        "                true_y = true_y.long().to(device)                \n",
        "                \n",
        "                output = model(data)\n",
        "                pred_y = torch.argmax(output, axis=1)\n",
        "                \n",
        "                loss = criterion(output, true_y)\n",
        "\n",
        "                pred_y_list.extend(pred_y.tolist())\n",
        "                true_y_list.extend(true_y.tolist())\n",
        "    train_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "    \n",
        "    return train_accuracy , loss.cpu().detach().numpy()\n",
        "\n",
        "def savemodel(model,epoch,acc):\n",
        "    now = datetime.now()\n",
        "    pth = \"/content/drive/MyDrive/11785/model/\"\n",
        "    name = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "    torch.save(model.state_dict(), f\"{pth}model_epoch_acc_{acc}_{epoch}_{name}.pth\")\n",
        "    # files.download(f\"{pth}model_epoch_{epoch}_{name}.pth\")\n",
        "\n",
        "\n",
        "def main(config=None):\n",
        "    args = {\n",
        "        'batch_size': 1024*4,\n",
        "        'context': 30,\n",
        "        'log_interval': 1000,\n",
        "        'LIBRI_PATH': './hw1p2_student_data',\n",
        "        'lr': 0.001,\n",
        "        'epoch': 200\n",
        "    }\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    with wandb.init(config=config):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "        # print(1)\n",
        "\n",
        "        # loader = build_dataset(config.batch_size)\n",
        "        print(f\"context {config.context}\")\n",
        "        network = build_network(config.context, config.dropout)\n",
        "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "\n",
        "        # print(2)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=\"/content/drive/MyDrive/11785/train_filenames_subset_8192_v2.csv\")\n",
        "        # train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=None)\n",
        "        dev_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n",
        "        # print(3)\n",
        "        \n",
        "        print(config.epochs)\n",
        "        for epoch in range(1, config.epochs + 1):\n",
        "            # print(4)\n",
        "            print(f\"now_lr: {scheduler.get_last_lr()}\")\n",
        "            train_loss = train(args, network, device, train_samples, optimizer,scheduler, criterion, epoch,config.batch_size)\n",
        "            # print(5)\n",
        "            test_acc, test_loss = test(args, network, device, dev_samples,criterion)\n",
        "            print('Dev accuracy ', test_acc)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                print(f\"lr: {param_group['lr']}\")\n",
        "            savemodel(network,epoch,test_acc*100)\n",
        "\n",
        "            wandb.log({\"loss\": test_loss, \"epoch\": epoch})\n",
        "            scheduler.step()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wandb Sweep\n",
        "Wandb sweep could auto sweep the parameters for you and log the results. However, I started to know the wandb after I got a 86 acc, thus, I did not use this sweep so much."
      ],
      "metadata": {
        "id": "I38c6D_ct1sQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl23Fnjb1nB4",
        "outputId": "6abc7fbe-1b84-430b-c9ea-a1a1c7821378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'method': 'random',\n",
            " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
            " 'parameters': {'batch_size': {'values': [8192]},\n",
            "                'context': {'values': [30]},\n",
            "                'dropout': {'values': [0.1]},\n",
            "                'epochs': {'value': 20},\n",
            "                'learning_rate': {'distribution': 'uniform',\n",
            "                                  'max': 0.1,\n",
            "                                  'min': 0.08},\n",
            "                'optimizer': {'values': ['adam']}}}\n",
            "Create sweep with ID: mrrwt8dv\n",
            "Sweep URL: https://wandb.ai/linjiw/pytorch-test1/sweeps/mrrwt8dv\n"
          ]
        }
      ],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'   \n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        'values': ['adam']\n",
        "        },\n",
        "    'dropout': {\n",
        "          'values': [0.1]\n",
        "        },\n",
        "    'context': {\n",
        "          'values': [30]\n",
        "        },\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "parameters_dict.update({\n",
        "    'epochs': {\n",
        "        'value': 20}\n",
        "    })\n",
        "\n",
        "import math\n",
        "\n",
        "parameters_dict.update({\n",
        "    'learning_rate': {\n",
        "        # a flat distribution between 0 and 0.1\n",
        "        'distribution': 'uniform',\n",
        "        'min': 0.08,\n",
        "        'max': 0.1\n",
        "      },\n",
        "    'batch_size': {\n",
        "        # integers between 32 and 256\n",
        "        # with evenly-distributed logarithms \n",
        "        'values': [512*16]\n",
        "      }\n",
        "    })\n",
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-test1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UG5Vj00JBnSP",
        "outputId": "00376cb5-568a-4a9e-a658-2e8ea22ea146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mn1ecptu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8192\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcontext: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08907761174737502\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/linjiw/pytorch-test1/runs/mn1ecptu\" target=\"_blank\">prime-sweep-1</a></strong> to <a href=\"https://wandb.ai/linjiw/pytorch-test1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/linjiw/pytorch-test1/sweeps/mrrwt8dv\" target=\"_blank\">https://wandb.ai/linjiw/pytorch-test1/sweeps/mrrwt8dv</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context 30\n",
            "20\n",
            "now_lr: [0.08907761174737502]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/10391263 (0%)]\tLoss: 4.676150\n",
            "Train Epoch: 1 [8192000/10391263 (79%)]\tLoss: 1.371432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.77s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.733108094158646\n",
            "lr: 0.08907761174737502\n",
            "now_lr: [0.08016985057263752]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 2 [0/10391263 (0%)]\tLoss: 1.307132\n",
            "Train Epoch: 2 [8192000/10391263 (79%)]\tLoss: 1.236490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.79s/it]\n",
            "100%|██████████| 1/1 [00:44<00:00, 44.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.7676046815064392\n",
            "lr: 0.08016985057263752\n",
            "now_lr: [0.07215286551537377]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 3 [0/10391263 (0%)]\tLoss: 1.209608\n",
            "Train Epoch: 3 [8192000/10391263 (79%)]\tLoss: 1.164493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.70s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.780425353136213\n",
            "lr: 0.07215286551537377\n",
            "now_lr: [0.0649375789638364]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 4 [0/10391263 (0%)]\tLoss: 1.115748\n",
            "Train Epoch: 4 [8192000/10391263 (79%)]\tLoss: 1.100595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.49s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.7852831696168663\n",
            "lr: 0.0649375789638364\n",
            "now_lr: [0.05844382106745276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 5 [0/10391263 (0%)]\tLoss: 1.091537\n",
            "Train Epoch: 5 [8192000/10391263 (79%)]\tLoss: 1.052114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.57s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.7872062703613324\n",
            "lr: 0.05844382106745276\n",
            "now_lr: [0.052599438960707484]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 6 [0/10391263 (0%)]\tLoss: 1.043231\n",
            "Train Epoch: 6 [8192000/10391263 (79%)]\tLoss: 1.024079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.58s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.7926968623419094\n",
            "lr: 0.052599438960707484\n",
            "now_lr: [0.047339495064636736]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 7 [0/10391263 (0%)]\tLoss: 0.993001\n",
            "Train Epoch: 7 [8192000/10391263 (79%)]\tLoss: 0.977029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.67s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.7941523492177532\n",
            "lr: 0.047339495064636736\n",
            "now_lr: [0.042605545558173065]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 8 [0/10391263 (0%)]\tLoss: 0.974417\n",
            "Train Epoch: 8 [8192000/10391263 (79%)]\tLoss: 0.942759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.87s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.7968935161672592\n",
            "lr: 0.042605545558173065\n",
            "now_lr: [0.038344991002355756]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 9 [0/10391263 (0%)]\tLoss: 0.916760\n",
            "Train Epoch: 9 [8192000/10391263 (79%)]\tLoss: 0.936138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.48s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.7983970031422001\n",
            "lr: 0.038344991002355756\n",
            "now_lr: [0.03451049190212018]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 10 [0/10391263 (0%)]\tLoss: 0.908958\n",
            "Train Epoch: 10 [8192000/10391263 (79%)]\tLoss: 0.917345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.66s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.7995299087069083\n",
            "lr: 0.03451049190212018\n",
            "now_lr: [0.031059442711908164]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 11 [0/10391263 (0%)]\tLoss: 0.865336\n",
            "Train Epoch: 11 [8192000/10391263 (79%)]\tLoss: 0.885617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.88s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.8009539116467853\n",
            "lr: 0.031059442711908164\n",
            "now_lr: [0.02795349844071735]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 12 [0/10391263 (0%)]\tLoss: 0.850448\n",
            "Train Epoch: 12 [8192000/10391263 (79%)]\tLoss: 0.857223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:23<00:00, 623.40s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.8015882355370024\n",
            "lr: 0.02795349844071735\n",
            "now_lr: [0.025158148596645613]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 13 [0/10391263 (0%)]\tLoss: 0.844762\n",
            "Train Epoch: 13 [8192000/10391263 (79%)]\tLoss: 0.843644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.88s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.8039577888160802\n",
            "lr: 0.025158148596645613\n",
            "now_lr: [0.022642333736981053]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 14 [0/10391263 (0%)]\tLoss: 0.825486\n",
            "Train Epoch: 14 [8192000/10391263 (79%)]\tLoss: 0.843708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.49s/it]\n",
            "100%|██████████| 1/1 [00:44<00:00, 44.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.8040238534686007\n",
            "lr: 0.022642333736981053\n",
            "now_lr: [0.02037810036328295]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 15 [0/10391263 (0%)]\tLoss: 0.783761\n",
            "Train Epoch: 15 [8192000/10391263 (79%)]\tLoss: 0.830573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.80s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.8045100480207443\n",
            "lr: 0.02037810036328295\n",
            "now_lr: [0.018340290326954653]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 16 [0/10391263 (0%)]\tLoss: 0.824297\n",
            "Train Epoch: 16 [8192000/10391263 (79%)]\tLoss: 0.796755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.94s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.8051066944138208\n",
            "lr: 0.018340290326954653\n",
            "now_lr: [0.016506261294259188]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 17 [0/10391263 (0%)]\tLoss: 0.794803\n",
            "Train Epoch: 17 [8192000/10391263 (79%)]\tLoss: 0.811939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [10:22<00:00, 622.61s/it]\n",
            "100%|██████████| 1/1 [00:43<00:00, 43.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy  0.805650179406822\n",
            "lr: 0.016506261294259188\n",
            "now_lr: [0.01485563516483327]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 18 [0/10391263 (0%)]\tLoss: 0.797178\n"
          ]
        }
      ],
      "source": [
        "wandb.agent(sweep_id, main, count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test and submit results to kaggle"
      ],
      "metadata": {
        "id": "kkmFIRJ5uZX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class test_sample(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"\", csvpath=None):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        self.sample = sample \n",
        "        \n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        \n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "\n",
        "        # using a small part of the dataset to debug\n",
        "        # if csvpath:\n",
        "        #     # subset = self.parse_csv(csvpath)\n",
        "        #     # self.X_names = [i for i in self.X_names if i in subset]\n",
        "        self.X_names = list(pd.read_csv(csvpath).file)\n",
        "        \n",
        "        # print(self.X_names)\n",
        "        \n",
        "        self.length = len(self.X_names)\n",
        "        \n",
        "        self.PHONEMES = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n",
        "      \n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[0])\n",
        "                # print(row[0])\n",
        "        return subset[0:]\n",
        "\n",
        "    def __len__(self):\n",
        "        # return int(np.ceil(self.length / self.sample))\n",
        "        return len(self.X_names)\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        # sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n",
        "        \n",
        "        X = []\n",
        "        for j in self.X_names:\n",
        "            X_path = self.X_dir + j\n",
        "            \n",
        "            # print(X_path)\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "            \n",
        "        X = np.concatenate(X)\n",
        "        return X\n",
        "    \n",
        "class test_item(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, context = 0):\n",
        "        \n",
        "        self.length  = X.shape[0]\n",
        "        self.context = context\n",
        "\n",
        "        self.context = context\n",
        "        empty_matrix = np.zeros((context,13))\n",
        "        if context == 0:\n",
        "            self.X = X\n",
        "        else:\n",
        "            # TODO: self.X, self.Y = ... \n",
        "            self.X = np.concatenate((empty_matrix,X,empty_matrix ))\n",
        "            pass \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        if self.context == 0:\n",
        "            xx = self.X[i].flatten()\n",
        "        else:\n",
        "            # TODO xx, yy = ...\n",
        "            xx = self.X[i:i+2*self.context+1].flatten()\n",
        "\n",
        "            pass\n",
        "        return xx\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "rshvXp_CuRiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def submit_test(args, model, device, test_samples):\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(1):\n",
        "            X = test_samples[i]\n",
        "\n",
        "            test_items = test_item(X, context=args['context'])\n",
        "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for data in test_loader:\n",
        "                data = data.float().to(device)\n",
        "                                \n",
        "                output = model(data)\n",
        "                pred_y = torch.argmax(output, axis=1)\n",
        "\n",
        "                pred_y_list.extend(pred_y.tolist())\n",
        "    # print(pred_y_list)\n",
        "    now_name = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "    f = open(f\"bad_{now_name}.csv\", \"w\")\n",
        "    f.write(\"id,label\\n\")\n",
        "    for idx, i  in enumerate(pred_y_list):\n",
        "        f.write(f\"{idx},{i}\\n\")\n",
        "    f.close()\n",
        "\n",
        " \n",
        "    # with open('good.csv', 'w', newline='') as csvfile:\n",
        "    #     writer = csv.DictWriter(csvfile, fieldnames = ['id','label'])\n",
        "    #     # writer.writerow(['id','label'])\n",
        "    #     writer.writeheader() \n",
        "    #     for idx, i  in enumerate(pred_y_list):\n",
        "    #         writer.writerow([idx,i])\n",
        "        \n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "EYh2l7c1uSPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def submit(model_name,model,args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.load_state_dict(torch.load(model_name))\n",
        "    model = model.to(device)\n",
        "    test_samples = test_sample(data_path = args['LIBRI_PATH'], shuffle=False, partition=\"test-clean\",csvpath='test_order.csv')\n",
        "    test_acc = submit_test(args, model, device, test_samples)\n",
        "\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "MLR67D01uVFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = submit(\"/content/drive/MyDrive/11785/model/model_epoch_11_04_02_2022_18_52_28.pth\",Network(args['context']),args)"
      ],
      "metadata": {
        "id": "lg8Qfk7suWuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions submit -c 11-785-s22-hw1p2 -f bad_04_02_2022_22_40_02.csv -m \"Message\""
      ],
      "metadata": {
        "id": "rA-Max7GuYCN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "HW1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFF2zczSXW/35RqjJbdh9U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}