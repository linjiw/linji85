{"cells":[{"cell_type":"code","source":[""],"metadata":{"id":"BAzIPemG6qC3"},"id":"BAzIPemG6qC3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Kng9Hrm6q6T","executionInfo":{"status":"ok","timestamp":1643645856086,"user_tz":300,"elapsed":14856,"user":{"displayName":"Joe Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDiMcYRC8LIlDF2Xa1XX9pk-WXcZ1xWFYmAZZX=s64","userId":"11084623136410812061"}},"outputId":"805e7c00-b6a8-4d60-fd36-652efa2ab48a"},"id":"-Kng9Hrm6q6T","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"MIby3J0IWvkY","metadata":{"id":"MIby3J0IWvkY"},"outputs":[],"source":["import os\n","import csv\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score\n","from datetime import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","\n","#!pip install --upgrade --force-reinstall --no-deps kaggle"]},{"cell_type":"code","execution_count":null,"id":"4c3ec869","metadata":{"id":"4c3ec869"},"outputs":[],"source":["now = datetime.now()"]},{"cell_type":"code","execution_count":null,"id":"66b5eb16","metadata":{"id":"66b5eb16"},"outputs":[],"source":["name = now.strftime(\"%d_%m_%Y_%H_%M_%S\")"]},{"cell_type":"code","execution_count":null,"id":"d4655aa0","metadata":{"id":"d4655aa0","outputId":"36eba58b-7f5e-4fc0-953e-96d6a52f09f7"},"outputs":[{"data":{"text/plain":["'28_01_2022_19_58_46'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["name"]},{"cell_type":"code","execution_count":null,"id":"952d2124","metadata":{"id":"952d2124"},"outputs":[],"source":["def layer_cal(K,N):\n","    lst = []\n","    clst = []\n","    for i in range(K):\n","        c = 2**(-i/2)\n","        # print(c)\n","        clst.append(c)\n","        lst.append(2**(c*N))\n","    return lst, clst"]},{"cell_type":"code","execution_count":null,"id":"1d95ed47","metadata":{"id":"1d95ed47","outputId":"44c9980e-12a7-4ef0-cedd-8f0ac59c012d"},"outputs":[{"name":"stdout","output_type":"stream","text":["lst [1.1150372599265312e+43, 2.748085447144845e+30, 3.3392173632851925e+21, 1657735035264938.0, 57785961645.41343, 40715292.40058258, 240387.10790184533, 6380.853579309165, 490.29287971766973, 79.88024523816364] \n"," clst [1.0, 0.7071067811865476, 0.5, 0.3535533905932738, 0.25, 0.1767766952966369, 0.125, 0.08838834764831845, 0.0625, 0.04419417382415922] \n"," log2N 8.159871336778389\n"]}],"source":["lst, clst = layer_cal(10,143)\n","# print(f\"lst {lst} \\n clst {clst} \\n log2N {np.log2(2*143)}\")\n","# print(layer_cal(10,143)[1])\n"]},{"cell_type":"code","execution_count":null,"id":"Qcny8yCsWxmq","metadata":{"id":"Qcny8yCsWxmq"},"outputs":[],"source":["class Network(torch.nn.Module):\n","    def __init__(self,context):\n","        super(Network, self).__init__()\n","        # TODO: Please try different architectures\n","        in_size = 13 * (1+2*context)\n","        # hidden_size = 2**(in_size-1)\n","        # hidden_size = []a = \n","        # layer_number = np.ceil(np.log2(in_size)*2)\n","        layer_number = 7\n","        hidden_size = [in_size,in_size*2]\n","        \n","        \n","        for i in range(1,layer_number):\n","            if i % 2 == 1:\n","                hidden_size.append(hidden_size[i]//2+1)\n","                # hidden_size.append(nn.BatchNorm1d(hidden_size[i]//2+1))\n","            else:\n","                hidden_size.append(hidden_size[i])\n","        # print(hidden_size)\n","        layers = []\n","        # layers.append()\n","        # sigmoid for starter input\n","        # Dropout ? need larger neurons\n","        # Batchnorm\n","        \n","        \n","        for i in range(len(hidden_size)-1):\n","            # print(i)\n","            layers.append(nn.Linear(hidden_size[i], hidden_size[i+1]))\n","            layers.append(nn.ReLU())\n","\n","            # if i== len(hidden_size)-2:\n","            #     layers.append(nn.Dropout(p=0.2))\n","                \n","                \n","            if i%2 == 0:\n","                layers.append(nn.Dropout(p=0.2))\n","                layers.append(nn.BatchNorm1d(hidden_size[i+1]))\n","            \n","        layers.append(nn.Linear(hidden_size[-1], 40))\n","        # layers.append(40)\n","\n","        # print(f\"in_size {in_size}; hidden_size: {hidden_size}\")\n","        # layers = [\n","        #     nn.Linear(in_size, hidden_size[0]),\n","        #     nn.ReLU(),\n","        #     nn.Linear(in_size, in_size//2+1),\n","        #     nn.ReLU(),\n","        #     nn.Linear(in_size//2+1, in_size//2+1),\n","        #     nn.ReLU(),\n","        #     nn.Linear(in_size//2+1, (in_size//2+1)),\n","        #     nn.ReLU(),\n","        #     nn.Linear(in_size, hidden_size),\n","        #     nn.ReLU(),\n","        #     nn.Linear(2**(in_size-1), 40)\n","        # ]\n","        self.laysers = nn.Sequential(*layers)\n","\n","    def forward(self, A0):\n","        x = self.laysers(A0)\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"b6667d0c","metadata":{"id":"b6667d0c"},"outputs":[],"source":["class Network(torch.nn.Module):\n","    def __init__(self,context):\n","        super(Network, self).__init__()\n","        # TODO: Please try different architectures\n","        in_size = 13 * (1+2*context)\n","        # hidden_size = 2**(in_size-1)\n","        # hidden_size = []a = \n","        # layer_number = np.ceil(np.log2(in_size)*2)\n","        layer_number = 6\n","        hidden_size = [in_size,in_size*2]\n","        \n","        \n","        for i in range(1,layer_number):\n","            # if i % 2 == 1:\n","            hidden_size.append(hidden_size[i]//2)\n","                # hidden_size.append(nn.BatchNorm1d(hidden_size[i]//2+1))\n","            # else:\n","                # hidden_size.append(hidden_size[i])\n","        # print(hidden_size)\n","        layers = []\n","        # layers.append()\n","        # sigmoid for starter input\n","        # Dropout ? need larger neurons\n","        # Batchnorm\n","        \n","        \n","        for i in range(len(hidden_size)-1):\n","            # print(i)\n","            layers.append(nn.Linear(hidden_size[i], hidden_size[i+1]))\n","            layers.append(nn.ReLU())\n","\n","            # if i== len(hidden_size)-2:\n","            #     layers.append(nn.Dropout(p=0.2))\n","                \n","                \n","            if i%2 == 0:\n","                layers.append(nn.Dropout(p=0.4))\n","                layers.append(nn.BatchNorm1d(hidden_size[i+1]))\n","            \n","        layers.append(nn.Linear(hidden_size[-1], 40))\n","        # layers.append(40)\n","\n","        # print(f\"in_size {in_size}; hidden_size: {hidden_size}\")\n","        # layers = [\n","        #     nn.Linear(in_size, hidden_size[0]),\n","        #     nn.ReLU(),\n","        #     nn.Linear(in_size, in_size//2+1),\n","        #     nn.ReLU(),\n","        #     nn.Linear(in_size//2+1, in_size//2+1),\n","        #     nn.ReLU(),\n","        #     nn.Linear(in_size//2+1, (in_size//2+1)),\n","        #     nn.ReLU(),\n","        #     nn.Linear(in_size, hidden_size),\n","        #     nn.ReLU(),\n","        #     nn.Linear(2**(in_size-1), 40)\n","        # ]\n","        self.laysers = nn.Sequential(*layers)\n","\n","    def forward(self, A0):\n","        x = self.laysers(A0)\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"c6ef899c","metadata":{"id":"c6ef899c","outputId":"e76e1429-0f26-4a45-a40e-162d69b81613"},"outputs":[{"data":{"text/plain":["Network(\n","  (laysers): Sequential(\n","    (0): Linear(in_features=793, out_features=1586, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.4, inplace=False)\n","    (3): BatchNorm1d(1586, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): Linear(in_features=1586, out_features=793, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=793, out_features=396, bias=True)\n","    (7): ReLU()\n","    (8): Dropout(p=0.4, inplace=False)\n","    (9): BatchNorm1d(396, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): Linear(in_features=396, out_features=198, bias=True)\n","    (11): ReLU()\n","    (12): Linear(in_features=198, out_features=99, bias=True)\n","    (13): ReLU()\n","    (14): Dropout(p=0.4, inplace=False)\n","    (15): BatchNorm1d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (16): Linear(in_features=99, out_features=49, bias=True)\n","    (17): ReLU()\n","    (18): Linear(in_features=49, out_features=40, bias=True)\n","  )\n",")"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["Network(30)"]},{"cell_type":"code","execution_count":null,"id":"Jr9VJwbGWxrY","metadata":{"id":"Jr9VJwbGWxrY"},"outputs":[],"source":["\n","class LibriSamples(torch.utils.data.Dataset):\n","    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None):\n","        # sample represent how many npy files will be preloaded for one __getitem__ call\n","        self.sample = sample \n","        \n","        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n","        self.Y_dir = data_path + \"/\" + partition +\"/transcript/\"\n","        \n","        self.X_names = os.listdir(self.X_dir)\n","        self.Y_names = os.listdir(self.Y_dir)\n","\n","        # using a small part of the dataset to debug\n","        if csvpath:\n","            subset = self.parse_csv(csvpath)\n","            self.X_names = [i for i in self.X_names if i in subset]\n","            self.Y_names = [i for i in self.Y_names if i in subset]\n","        \n","        if shuffle == True:\n","            XY_names = list(zip(self.X_names, self.Y_names))\n","            random.shuffle(XY_names)\n","            self.X_names, self.Y_names = zip(*XY_names)\n","        \n","        assert(len(self.X_names) == len(self.Y_names))\n","        self.length = len(self.X_names)\n","        \n","        self.PHONEMES = [\n","            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n","            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n","            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n","            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n","            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n","            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n","      \n","    @staticmethod\n","    def parse_csv(filepath):\n","        subset = []\n","        with open(filepath) as f:\n","            f_csv = csv.reader(f)\n","            for row in f_csv:\n","                subset.append(row[1])\n","        return subset[1:]\n","\n","    def __len__(self):\n","        return int(np.ceil(self.length / self.sample))\n","        \n","    def __getitem__(self, i):\n","        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n","        \n","        X, Y = [], []\n","        for j in sample_range:\n","            X_path = self.X_dir + self.X_names[j]\n","            Y_path = self.Y_dir + self.Y_names[j]\n","            \n","            label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n","\n","            X_data = np.load(X_path)\n","            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n","            X.append(X_data)\n","            Y.append(np.array(label))\n","            \n","        X, Y = np.concatenate(X), np.concatenate(Y)\n","        return X, Y\n","    \n","class LibriItems(torch.utils.data.Dataset):\n","    def __init__(self, X, Y, context = 0):\n","        assert(X.shape[0] == Y.shape[0])\n","        \n","        self.length  = X.shape[0]\n","        # print(f\"self.length {self.length}\")\n","        self.context = context\n","        empty_matrix = np.zeros((context,13))\n","        if context == 0:\n","            self.X, self.Y = X, Y\n","        else:\n","            # TODO: self.X, self.Y = ... \n","            self.X = np.concatenate((empty_matrix,X,empty_matrix ))\n","            self.Y = Y\n","            pass \n","        \n","    def __len__(self):\n","        return self.length\n","        \n","    def __getitem__(self, i):\n","        if self.context == 0:\n","            xx = self.X[i].flatten()\n","            yy = self.Y[i]\n","        else:\n","            # TODO xx, yy = ...\n","            # xx = self.X[self.context:]\n","            xx = self.X[i:i+2*self.context+1].flatten()\n","            # print(f\"xx.shape {xx.shape} pos {i}\")\n","\n","            yy = self.Y[i]\n","            pass\n","        return xx, yy\n","    \n","\n"]},{"cell_type":"code","execution_count":null,"id":"cbbc1147","metadata":{"id":"cbbc1147"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"2cbb1d57","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cbb1d57","outputId":"69aa0e1f-6918-44e4-ce3b-e96129129774"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","Train Epoch: 1 [0/25357288 (0%)]\tLoss: 3.685984\n","Train Epoch: 1 [10240000/25357288 (40%)]\tLoss: 1.068380\n","Train Epoch: 1 [20480000/25357288 (81%)]\tLoss: 0.852153\n","Train Epoch: 1 [0/10833846 (0%)]\tLoss: 1.003606\n","Train Epoch: 1 [10240000/10833846 (95%)]\tLoss: 0.873816\n","Dev accuracy  0.7661966785995945\n","Train Epoch: 2 [0/25357288 (0%)]\tLoss: 0.892969\n","Train Epoch: 2 [10240000/25357288 (40%)]\tLoss: 0.745622\n","Train Epoch: 2 [20480000/25357288 (81%)]\tLoss: 0.841479\n","Train Epoch: 2 [0/10833846 (0%)]\tLoss: 0.863091\n","Train Epoch: 2 [10240000/10833846 (95%)]\tLoss: 0.812766\n","Dev accuracy  0.7852047178419981\n","Train Epoch: 3 [0/25357288 (0%)]\tLoss: 0.783799\n","Train Epoch: 3 [10240000/25357288 (40%)]\tLoss: 0.752581\n","Train Epoch: 3 [20480000/25357288 (81%)]\tLoss: 0.751160\n","Train Epoch: 3 [0/10833846 (0%)]\tLoss: 0.786152\n","Train Epoch: 3 [10240000/10833846 (95%)]\tLoss: 0.760624\n","Dev accuracy  0.7945569952144418\n","Train Epoch: 4 [0/25357288 (0%)]\tLoss: 0.732496\n","Train Epoch: 4 [10240000/25357288 (40%)]\tLoss: 0.725072\n","Train Epoch: 4 [20480000/25357288 (81%)]\tLoss: 0.795737\n","Train Epoch: 4 [0/10833846 (0%)]\tLoss: 0.790760\n","Train Epoch: 4 [10240000/10833846 (95%)]\tLoss: 0.754984\n","Dev accuracy  0.7995613926428752\n","Train Epoch: 5 [0/25357288 (0%)]\tLoss: 0.708213\n","Train Epoch: 5 [10240000/25357288 (40%)]\tLoss: 0.808561\n","Train Epoch: 5 [20480000/25357288 (81%)]\tLoss: 0.753468\n","Train Epoch: 5 [0/10833846 (0%)]\tLoss: 0.703066\n","Train Epoch: 5 [10240000/10833846 (95%)]\tLoss: 0.660539\n","Dev accuracy  0.8030917225119433\n","Train Epoch: 6 [0/25357288 (0%)]\tLoss: 0.707984\n","Train Epoch: 6 [10240000/25357288 (40%)]\tLoss: 0.742621\n","Train Epoch: 6 [20480000/25357288 (81%)]\tLoss: 0.758828\n","Train Epoch: 6 [0/10833846 (0%)]\tLoss: 0.736898\n","Train Epoch: 6 [10240000/10833846 (95%)]\tLoss: 0.647589\n","Dev accuracy  0.8061740514561062\n","Train Epoch: 7 [0/25357288 (0%)]\tLoss: 0.711075\n","Train Epoch: 7 [10240000/25357288 (40%)]\tLoss: 0.724294\n","Train Epoch: 7 [20480000/25357288 (81%)]\tLoss: 0.644896\n","Train Epoch: 7 [0/10833846 (0%)]\tLoss: 0.746032\n","Train Epoch: 7 [10240000/10833846 (95%)]\tLoss: 0.661977\n","Dev accuracy  0.8081962491793532\n","Train Epoch: 8 [0/25357288 (0%)]\tLoss: 0.738350\n","Train Epoch: 8 [10240000/25357288 (40%)]\tLoss: 0.675495\n","Train Epoch: 8 [20480000/25357288 (81%)]\tLoss: 0.668306\n","Train Epoch: 8 [0/10833846 (0%)]\tLoss: 0.674825\n","Train Epoch: 8 [10240000/10833846 (95%)]\tLoss: 0.702226\n","Dev accuracy  0.8100563820518856\n","Train Epoch: 9 [0/25357288 (0%)]\tLoss: 0.680774\n","Train Epoch: 9 [10240000/25357288 (40%)]\tLoss: 0.686074\n","Train Epoch: 9 [20480000/25357288 (81%)]\tLoss: 0.655894\n","Train Epoch: 9 [0/10833846 (0%)]\tLoss: 0.658340\n","Train Epoch: 9 [10240000/10833846 (95%)]\tLoss: 0.705713\n","Dev accuracy  0.812051741010046\n","Train Epoch: 10 [0/25357288 (0%)]\tLoss: 0.708649\n","Train Epoch: 10 [10240000/25357288 (40%)]\tLoss: 0.684371\n","Train Epoch: 10 [20480000/25357288 (81%)]\tLoss: 0.709445\n","Train Epoch: 10 [0/10833846 (0%)]\tLoss: 0.735107\n","Train Epoch: 10 [10240000/10833846 (95%)]\tLoss: 0.740277\n","Dev accuracy  0.8132976790661761\n","Train Epoch: 11 [0/25357288 (0%)]\tLoss: 0.658699\n","Train Epoch: 11 [10240000/25357288 (40%)]\tLoss: 0.629356\n","Train Epoch: 11 [20480000/25357288 (81%)]\tLoss: 0.644905\n","Train Epoch: 11 [0/10833846 (0%)]\tLoss: 0.692945\n","Train Epoch: 11 [10240000/10833846 (95%)]\tLoss: 0.685179\n","Dev accuracy  0.8141818099237366\n","Train Epoch: 12 [0/25357288 (0%)]\tLoss: 0.668878\n","Train Epoch: 12 [10240000/25357288 (40%)]\tLoss: 0.669877\n","Train Epoch: 12 [20480000/25357288 (81%)]\tLoss: 0.643107\n","Train Epoch: 12 [0/10833846 (0%)]\tLoss: 0.716627\n","Train Epoch: 12 [10240000/10833846 (95%)]\tLoss: 0.598540\n","Dev accuracy  0.8156997485414164\n","Train Epoch: 13 [0/25357288 (0%)]\tLoss: 0.597633\n","Train Epoch: 13 [10240000/25357288 (40%)]\tLoss: 0.623806\n","Train Epoch: 13 [20480000/25357288 (81%)]\tLoss: 0.715788\n","Train Epoch: 13 [0/10833846 (0%)]\tLoss: 0.625129\n","Train Epoch: 13 [10240000/10833846 (95%)]\tLoss: 0.667787\n","Dev accuracy  0.8166649118243341\n","Train Epoch: 14 [0/25357288 (0%)]\tLoss: 0.633253\n","Train Epoch: 14 [10240000/25357288 (40%)]\tLoss: 0.625711\n","Train Epoch: 14 [20480000/25357288 (81%)]\tLoss: 0.733892\n","Train Epoch: 14 [0/10833846 (0%)]\tLoss: 0.684273\n","Train Epoch: 14 [10240000/10833846 (95%)]\tLoss: 0.693646\n","Dev accuracy  0.8171082675783589\n","Train Epoch: 15 [0/25357288 (0%)]\tLoss: 0.642735\n","Train Epoch: 15 [10240000/25357288 (40%)]\tLoss: 0.631035\n","Train Epoch: 15 [20480000/25357288 (81%)]\tLoss: 0.702757\n","Train Epoch: 15 [0/10833846 (0%)]\tLoss: 0.618009\n","Train Epoch: 15 [10240000/10833846 (95%)]\tLoss: 0.670819\n","Dev accuracy  0.8181854310925029\n","Train Epoch: 16 [0/25357288 (0%)]\tLoss: 0.685567\n","Train Epoch: 16 [10240000/25357288 (40%)]\tLoss: 0.639174\n","Train Epoch: 16 [20480000/25357288 (81%)]\tLoss: 0.630379\n","Train Epoch: 16 [0/10833846 (0%)]\tLoss: 0.727404\n","Train Epoch: 16 [10240000/10833846 (95%)]\tLoss: 0.665281\n","Dev accuracy  0.8188569163497628\n","Train Epoch: 17 [0/25357288 (0%)]\tLoss: 0.665407\n","Train Epoch: 17 [10240000/25357288 (40%)]\tLoss: 0.656926\n","Train Epoch: 17 [20480000/25357288 (81%)]\tLoss: 0.622094\n","Train Epoch: 17 [0/10833846 (0%)]\tLoss: 0.646372\n","Train Epoch: 17 [10240000/10833846 (95%)]\tLoss: 0.608920\n","Dev accuracy  0.8200512413961113\n","Train Epoch: 18 [0/25357288 (0%)]\tLoss: 0.600084\n","Train Epoch: 18 [10240000/25357288 (40%)]\tLoss: 0.634066\n","Train Epoch: 18 [20480000/25357288 (81%)]\tLoss: 0.718150\n","Train Epoch: 18 [0/10833846 (0%)]\tLoss: 0.679012\n","Train Epoch: 18 [10240000/10833846 (95%)]\tLoss: 0.624446\n","Dev accuracy  0.8197193697432149\n","Train Epoch: 19 [0/25357288 (0%)]\tLoss: 0.696108\n","Train Epoch: 19 [10240000/25357288 (40%)]\tLoss: 0.667872\n","Train Epoch: 19 [20480000/25357288 (81%)]\tLoss: 0.657943\n","Train Epoch: 19 [0/10833846 (0%)]\tLoss: 0.652902\n","Train Epoch: 19 [10240000/10833846 (95%)]\tLoss: 0.578369\n","Dev accuracy  0.8208352430146952\n","Train Epoch: 20 [0/25357288 (0%)]\tLoss: 0.635632\n","Train Epoch: 20 [10240000/25357288 (40%)]\tLoss: 0.586331\n","Train Epoch: 20 [20480000/25357288 (81%)]\tLoss: 0.714357\n","Train Epoch: 20 [0/10833846 (0%)]\tLoss: 0.668709\n","Train Epoch: 20 [10240000/10833846 (95%)]\tLoss: 0.660586\n","Dev accuracy  0.821501566970977\n","Train Epoch: 21 [0/25357288 (0%)]\tLoss: 0.643052\n","Train Epoch: 21 [10240000/25357288 (40%)]\tLoss: 0.613002\n","Train Epoch: 21 [20480000/25357288 (81%)]\tLoss: 0.565154\n","Train Epoch: 21 [0/10833846 (0%)]\tLoss: 0.641870\n","Train Epoch: 21 [10240000/10833846 (95%)]\tLoss: 0.630495\n","Dev accuracy  0.8218675032103292\n","Train Epoch: 22 [0/25357288 (0%)]\tLoss: 0.604866\n","Train Epoch: 22 [10240000/25357288 (40%)]\tLoss: 0.650834\n","Train Epoch: 22 [20480000/25357288 (81%)]\tLoss: 0.649038\n","Train Epoch: 22 [0/10833846 (0%)]\tLoss: 0.711235\n","Train Epoch: 22 [10240000/10833846 (95%)]\tLoss: 0.632674\n","Dev accuracy  0.8222788588982893\n","Train Epoch: 23 [0/25357288 (0%)]\tLoss: 0.700717\n","Train Epoch: 23 [10240000/25357288 (40%)]\tLoss: 0.594785\n","Train Epoch: 23 [20480000/25357288 (81%)]\tLoss: 0.687982\n","Train Epoch: 23 [0/10833846 (0%)]\tLoss: 0.635694\n","Train Epoch: 23 [10240000/10833846 (95%)]\tLoss: 0.652627\n","Dev accuracy  0.8228564084777465\n","Train Epoch: 24 [0/25357288 (0%)]\tLoss: 0.577793\n","Train Epoch: 24 [10240000/25357288 (40%)]\tLoss: 0.719254\n","Train Epoch: 24 [20480000/25357288 (81%)]\tLoss: 0.625220\n","Train Epoch: 24 [0/10833846 (0%)]\tLoss: 0.615900\n","Train Epoch: 24 [10240000/10833846 (95%)]\tLoss: 0.687920\n","Dev accuracy  0.8231480219830132\n","Train Epoch: 25 [0/25357288 (0%)]\tLoss: 0.572631\n","Train Epoch: 25 [10240000/25357288 (40%)]\tLoss: 0.651336\n","Train Epoch: 25 [20480000/25357288 (81%)]\tLoss: 0.655614\n","Train Epoch: 25 [0/10833846 (0%)]\tLoss: 0.668985\n","Train Epoch: 25 [10240000/10833846 (95%)]\tLoss: 0.688700\n","Dev accuracy  0.8238706041199569\n","Train Epoch: 26 [0/25357288 (0%)]\tLoss: 0.663218\n","Train Epoch: 26 [10240000/25357288 (40%)]\tLoss: 0.678211\n","Train Epoch: 26 [20480000/25357288 (81%)]\tLoss: 0.656321\n","Train Epoch: 26 [0/10833846 (0%)]\tLoss: 0.689372\n","Train Epoch: 26 [10240000/10833846 (95%)]\tLoss: 0.556626\n","Dev accuracy  0.824000152774509\n","Train Epoch: 27 [0/25357288 (0%)]\tLoss: 0.667996\n","Train Epoch: 27 [10240000/25357288 (40%)]\tLoss: 0.638602\n","Train Epoch: 27 [20480000/25357288 (81%)]\tLoss: 0.615292\n"]}],"source":["\n","\n","def train(args, model, device, train_samples, optimizer,scheduler, criterion, epoch):\n","    model.train()\n","    # print(f\"train_samples.shape: {train_samples.shape}\")\n","\n","    for i in range(len(train_samples)):\n","        X, Y = train_samples[i]\n","        train_items = LibriItems(X, Y, context=args['context'])\n","        train_loader = torch.utils.data.DataLoader(train_items, batch_size=args['batch_size'], shuffle=True)\n","        # print(f\"X.shape: {X.shape}\")\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            # print(f\"batch_idx: {batch_idx} data {data.shape}\")\n","            data = data.float().to(device)\n","            target = target.long().to(device)\n","\n","            optimizer.zero_grad()\n","            \n","            output = model(data)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","            if batch_idx % args['log_interval'] == 0:\n","                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                    epoch, batch_idx * len(data), len(train_loader.dataset),\n","                    100. * batch_idx / len(train_loader), loss.item()))\n","    # scheduler.step()\n","    return loss.cpu().detach().numpy()\n","\n","\n","def test(args, model, device, dev_samples,criterion):\n","    model.eval()\n","    true_y_list = []\n","    pred_y_list = []\n","    with torch.no_grad():\n","        for i in range(len(dev_samples)):\n","            X, Y = dev_samples[i]\n","\n","            test_items = LibriItems(X, Y, context=args['context'])\n","            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n","\n","            for data, true_y in test_loader:\n","                data = data.float().to(device)\n","                true_y = true_y.long().to(device)                \n","                \n","                output = model(data)\n","                pred_y = torch.argmax(output, axis=1)\n","                \n","                loss = criterion(output, true_y)\n","\n","                pred_y_list.extend(pred_y.tolist())\n","                true_y_list.extend(true_y.tolist())\n","    train_accuracy =  accuracy_score(true_y_list, pred_y_list)\n","    \n","    return train_accuracy , loss.cpu().detach().numpy()\n","\n","def savemodel(model,epoch):\n","    now = datetime.now()\n","    name = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n","    torch.save(model.state_dict(), f\"model_epoch_{epoch}_{name}.pth\")\n","    \n","\n","\n","def main(args):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(device)\n","    model = Network(args['context']).to(device)\n","    # optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n","    \n","    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=0.9)\n","    \n","    scheduler1 = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","    \n","\n","\n","    criterion = torch.nn.CrossEntropyLoss()\n","    # If you want to use full Dataset, please pass None to csvpath\n","    # train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=\"./content/train_filenames_subset_8192_v2.csv\")\n","    train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\")\n","\n","    # train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=\"./content/train_filenames_subset_0008_v2.csv\")\n","    dev_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n","    writer = SummaryWriter()\n","    for epoch in range(1, args['epoch'] + 1):\n","        train_loss = train(args, model, device, train_samples, optimizer,scheduler1, criterion, epoch,)\n","        test_acc, test_loss = test(args, model, device, dev_samples,criterion)\n","        print('Dev accuracy ', test_acc)\n","        savemodel(model,epoch)\n","        writer.add_scalar('Loss/train', train_loss, epoch)\n","        writer.add_scalar('Loss/test', train_loss, epoch)\n","\n","        writer.add_scalar('Acc/test', test_acc, epoch)\n","if __name__ == '__main__':\n","    args = {\n","        'batch_size': 1024,\n","        'context': 30,\n","        'log_interval': 10000,\n","        'LIBRI_PATH': './content',\n","        'lr': 0.02,\n","        'epoch': 200\n","    }\n","    main(args)"]},{"cell_type":"code","execution_count":null,"id":"0d7be50a","metadata":{"id":"0d7be50a"},"outputs":[],"source":["\n","class test_sample(torch.utils.data.Dataset):\n","    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"\", csvpath=None):\n","        # sample represent how many npy files will be preloaded for one __getitem__ call\n","        self.sample = sample \n","        \n","        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n","        \n","        self.X_names = os.listdir(self.X_dir)\n","\n","        # using a small part of the dataset to debug\n","        if csvpath:\n","            subset = self.parse_csv(csvpath)\n","            self.X_names = [i for i in self.X_names if i in subset]\n","        \n","        \n","        \n","        self.length = len(self.X_names)\n","        \n","        self.PHONEMES = [\n","            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n","            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n","            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n","            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n","            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n","            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n","      \n","    @staticmethod\n","    def parse_csv(filepath):\n","        subset = []\n","        with open(filepath) as f:\n","            f_csv = csv.reader(f)\n","            for row in f_csv:\n","                subset.append(row[1])\n","        return subset[1:]\n","\n","    def __len__(self):\n","        return int(np.ceil(self.length / self.sample))\n","        \n","    def __getitem__(self, i):\n","        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n","        \n","        X = []\n","        for j in sample_range:\n","            X_path = self.X_dir + self.X_names[j]\n","            \n","\n","            X_data = np.load(X_path)\n","            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n","            X.append(X_data)\n","            \n","        X = np.concatenate(X)\n","        return X\n","    \n","class test_item(torch.utils.data.Dataset):\n","    def __init__(self, X, context = 0):\n","        \n","        self.length  = X.shape[0]\n","        self.context = context\n","\n","        self.context = context\n","        empty_matrix = np.zeros((context,13))\n","        if context == 0:\n","            self.X = X\n","        else:\n","            # TODO: self.X, self.Y = ... \n","            self.X = np.concatenate((empty_matrix,X,empty_matrix ))\n","            pass \n","        \n","    def __len__(self):\n","        return self.length\n","        \n","    def __getitem__(self, i):\n","        if self.context == 0:\n","            xx = self.X[i].flatten()\n","        else:\n","            # TODO xx, yy = ...\n","            xx = self.X[i:i+2*self.context+1].flatten()\n","\n","            pass\n","        return xx\n","    \n","\n"]},{"cell_type":"code","execution_count":null,"id":"5881b252","metadata":{"id":"5881b252"},"outputs":[],"source":["args = {\n","        'batch_size': 1024,\n","        'context': 20,\n","        'log_interval': 100,\n","        'LIBRI_PATH': './content',\n","        'lr': 0.05,\n","        'epoch': 30\n","    }"]},{"cell_type":"code","execution_count":null,"id":"abb6733a","metadata":{"id":"abb6733a"},"outputs":[],"source":["def submit_test(args, model, device, test_samples):\n","    model.eval()\n","    true_y_list = []\n","    pred_y_list = []\n","    with torch.no_grad():\n","        for i in range(len(test_samples)):\n","            X = test_samples[i]\n","\n","            test_items = test_item(X, context=args['context'])\n","            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n","\n","            for data in test_loader:\n","                data = data.float().to(device)\n","                                \n","                output = model(data)\n","                pred_y = torch.argmax(output, axis=1)\n","\n","                pred_y_list.extend(pred_y.tolist())\n","    # print(pred_y_list)\n","    now_name = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n","    f = open(f\"bad_{now_name}.csv\", \"w\")\n","    f.write(\"id,label\\n\")\n","    for idx, i  in enumerate(pred_y_list):\n","        f.write(f\"{idx},{i}\\n\")\n","    f.close()\n","\n"," \n","    # with open('good.csv', 'w', newline='') as csvfile:\n","    #     writer = csv.DictWriter(csvfile, fieldnames = ['id','label'])\n","    #     # writer.writerow(['id','label'])\n","    #     writer.writeheader() \n","    #     for idx, i  in enumerate(pred_y_list):\n","    #         writer.writerow([idx,i])\n","        \n","    \n","    \n"]},{"cell_type":"code","execution_count":null,"id":"f6c5ab9e","metadata":{"id":"f6c5ab9e"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"7fdb795b","metadata":{"id":"7fdb795b"},"outputs":[],"source":["def submit(model_name,model,args):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.load_state_dict(torch.load(model_name))\n","    model = model.to(device)\n","    test_samples = test_sample(data_path = args['LIBRI_PATH'], shuffle=False, partition=\"test-clean\")\n","    test_acc = submit_test(args, model, device, test_samples)\n","\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"id":"9facec7f","metadata":{"id":"9facec7f"},"outputs":[],"source":["model = submit(\"model_epoch_100_30_01_2022_16_01_13.pth\",Network(20),args)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"Copy of hw1p2_s22_starter_notebook.ipynb","provenance":[]},"interpreter":{"hash":"239d5711fd35d2bb9bad2d5ca41e22b79107b4494fe73df9033b517b748af271"},"kernelspec":{"display_name":"PyTorch (3.8)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}